{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XtlpcfSz80XH"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-75760eee761f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm-gUxWzI6DN"
   },
   "source": [
    "# Load data with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU0KC68wWysN"
   },
   "outputs": [],
   "source": [
    "# Set up config variables\n",
    "config = {\n",
    "    'data_path': './data', # directory path of dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeGPybKkJ9TO"
   },
   "outputs": [],
   "source": [
    "# Import MNIST digit dataset\n",
    "def importMnistDataset(root, train, transform=[]):\n",
    "  # it will try to download the dataset if dataset is not found under the root directory \n",
    "  return torchvision.datasets.MNIST(root=root, train=train, download=True, transform=transform) \n",
    "\n",
    "# ImportMnistDataset may fail to download because of HTTP error (happens a lot recently).\n",
    "# We can manually download the datasets with the following comments,\n",
    "# If data_path value is changed in config, update it here as well.\n",
    "!wget -nc www.di.ens.fr/~lelarge/MNIST.tar.gz -P ./tmp\n",
    "!mkdir -p ./data/\n",
    "!tar -zxvf ./tmp/MNIST.tar.gz -C ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJfhe4dTMcjU"
   },
   "outputs": [],
   "source": [
    "# After download finished, run the code below. \n",
    "    \n",
    "# Test dataset contains 10,000 images\n",
    "# Train dataset contains 60,000 images\n",
    "train_set = importMnistDataset(root=config['data_path'], train=True)\n",
    "test_set = importMnistDataset(root=config['data_path'], train=False)\n",
    "print(train_set)\n",
    "print(test_set)\n",
    "del test_set, train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASdqzh_wNC-U"
   },
   "outputs": [],
   "source": [
    "# To add noise to images, we can take advantage of the transform module.\n",
    "# We have defined custom transform class Noise to add noise during image transformation\n",
    "class Noise(object):\n",
    "    # Here we create a noisy version of the data set\n",
    "    # The way that we do it is we go over all the pixels of\n",
    "    # each of the data points; then with probability p we multiply\n",
    "    # the value of that pixel by 0 (making it essentially black).\n",
    "    # Otherwise (with probability 1-p) we multiply the value of that\n",
    "    # pixel by 1 (essentially keeping the pixel untouched)\n",
    "\n",
    "    # drop_probability is basically the probability of dropping a pixel (p in the above)\n",
    "    # This is how we create the noisy data set.\n",
    "    # Convert image_set to a numpy array\n",
    "    def __init__(self, drop_probability=0):\n",
    "        self.drop_probability = drop_probability\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        n = torch.from_numpy(np.random.choice([0, 1], size=tensor.size(), p=[self.drop_probability, 1-self.drop_probability])) \n",
    "        return tensor * n\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(drop_probability={0})'.format(self.drop_probability)\n",
    "\n",
    "# Return transform function to convert an image into a tensor.\n",
    "# Add noise if drop_probability is provided.\n",
    "def generateTransform(drop_probability):\n",
    "  if drop_probability is not None and drop_probability > 0:\n",
    "    trans_noise = transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              Noise(drop_probability)\n",
    "                              ])\n",
    "    return trans_noise\n",
    "  else:\n",
    "    return transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6VcyFgrj0NI"
   },
   "source": [
    "In the following we print some information about the data set. Then we will visualize some noisy digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gndbyZwefaYo"
   },
   "outputs": [],
   "source": [
    "# Load training dataset and add noise\n",
    "train_set = importMnistDataset(config['data_path'], True, generateTransform(drop_probability=0.7))\n",
    "\n",
    "# Add dataset to pytorch DataLoader with mini-batch size 64 s.t. each batch contains 64 images.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get first batch of images and labels\n",
    "train_image_batch, classe_set = iter(train_loader).next()\n",
    "\n",
    "print(f'train_loader contains {len(train_loader)} batches of data.')\n",
    "print(f'train_image_batch has shape {train_image_batch.shape},')\n",
    "print('where 64 is the number of images in a batch, 1 is the number of image channels (1 for grayscale image),\\\n",
    " 28X28 stands for WxH (width and height of a single image).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P28LIVHGFB1g"
   },
   "outputs": [],
   "source": [
    "def show_gray_digits(image_set, row=2, col=3):\n",
    "  # Here we visualize some of the data points in the data set. \n",
    "  # Create a large figure, to be filled with multiple subplots.\n",
    "\n",
    "  # Since image_set is a tensor variable, we transform it to a numpy type variable.\n",
    "  image_set = image_set.detach().numpy()\n",
    "\n",
    "  for i in range(row*col):\n",
    "    # define subplot\n",
    "    plt.subplot(row, col, i+1)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(image_set[i,0], cmap=plt.get_cmap('gray'))\n",
    "  # show the figure\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opQYzVeMf8-S"
   },
   "outputs": [],
   "source": [
    "# Display noised images and their corresponding labels.\n",
    "show_gray_digits(train_image_batch, 2, 3)\n",
    "print(classe_set[0:6])\n",
    "del train_image_batch, classe_set, train_set, train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAg1YeXzkWQZ"
   },
   "source": [
    "Let us formally define functions to load data. The goal is to train a network that, given a noisy image, recovers the original image. Therefore, each training point consists of the input (noisy image) and the expected output (true image). Also, we will use only a small portion of the training data to make the task more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g5AsuZgeo7P"
   },
   "outputs": [],
   "source": [
    "def load_data(path, drop_probability, split_ratio, batch_size):  \n",
    "  # Import MNIST train and test datasets.\n",
    "  # Import train set without adding noise.\n",
    "  train_set = importMnistDataset(path, True, generateTransform(0))\n",
    "\n",
    "  # Use only the first 800 points for training\n",
    "  train_set = torch.utils.data.Subset(train_set, list(range(1, 800)))\n",
    "\n",
    "  # Import train set and add noise.\n",
    "  train_set_noise = importMnistDataset(path, True, generateTransform(drop_probability))\n",
    "\n",
    "  # Use only the first 800 points for training\n",
    "  train_set_noise = torch.utils.data.Subset(train_set_noise, list(range(1, 800)))\n",
    "\n",
    "  # Load the whole test dataset\n",
    "  test_set = importMnistDataset(path, False, generateTransform(0))\n",
    "  test_set_noise = importMnistDataset(path, False, generateTransform(drop_probability))\n",
    "\n",
    "  # Create a new dataset storing image pairs,\n",
    "  # an item in the dataset is a pair of images (original and noised).\n",
    "  train_set = PairDataset(train_set, train_set_noise)\n",
    "  test_set = PairDataset(test_set, test_set_noise)\n",
    "\n",
    "  # Further split train dataset into training and validation datasets.   \n",
    "  # Use validation dataset to get early feedback without peeking test dataset.                                   \n",
    "  train_set, val_set = split_dataSet(train_set, split_ratio)\n",
    "\n",
    "  # Generate train, validation and test dataloaders,\n",
    "  # Dataloader is used to loop through data batches.\n",
    "  train_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "  val_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=False)\n",
    "  test_loader = torch.utils.data.DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "  return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rjIPXFko70W"
   },
   "outputs": [],
   "source": [
    "# Partitions a dataset into two sets based on the split ratio. \n",
    "# e.g. 0.8 means 80% data in the first set and 20% in the second.\n",
    "# This will be used to split the training data into \"training\" and \"validation\" sets\n",
    "def split_dataSet(dataset, split_ratio):\n",
    "  a_size = int(split_ratio * len(dataset))\n",
    "  b_size = len(dataset) - a_size\n",
    "  a_set, b_set = random_split(dataset, [a_size, b_size]) \n",
    "  return a_set, b_set\n",
    "  \n",
    "# When get an item from the dataset, it returns a pair of data.\n",
    "# In our case, it returns image and corresponding noised image.\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, dataset_origin, dataset_noisy):\n",
    "        self.dataset_origin = dataset_origin\n",
    "        self.dataset_noisy = dataset_noisy\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.dataset_origin[index]\n",
    "        x2 = self.dataset_noisy[index]\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTH7wjPUqc2-"
   },
   "source": [
    "Let us load all the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8vU5ebBqkWF"
   },
   "outputs": [],
   "source": [
    "# Load train, validation and test data.\n",
    "train_loader, val_loader, test_loader = load_data(\n",
    "    path=config['data_path'], drop_probability=0.7, split_ratio=0.5, batch_size=64)\n",
    "print(f'train_loader has {len(train_loader)} batches')\n",
    "print(f'val_loader has {len(val_loader)} batches')\n",
    "print(f'test_loader has {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5s2Uq1rul_T"
   },
   "outputs": [],
   "source": [
    "# print(np.ceil(800*0.5/64)) #7\n",
    "# print(np.ceil(800*(1-0.5)/64)) #7\n",
    "# print(np.ceil(10000/64)) #157"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-NPWO3pvesa"
   },
   "source": [
    "Let us visualize images in each dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3m04eSnszAZ"
   },
   "outputs": [],
   "source": [
    "# Get a batch of images from each dataloader\n",
    "train_image_batch, train_noise_image_batch = iter(train_loader).next()\n",
    "val_image_batch, val_noise_image_batch = iter(val_loader).next()\n",
    "test_image_batch, test_noise_image_batch = iter(test_loader).next()\n",
    "\n",
    "# Show the first 3 image pairs\n",
    "print('train')\n",
    "show_gray_digits(train_image_batch[0], row=1, col=3)\n",
    "show_gray_digits(train_noise_image_batch[0], 1, 3)\n",
    "print('validation')\n",
    "show_gray_digits(val_image_batch[0], 1, 3)\n",
    "show_gray_digits(val_noise_image_batch[0], 1, 3)\n",
    "print('test')\n",
    "show_gray_digits(test_image_batch[0], 1, 3)\n",
    "show_gray_digits(test_noise_image_batch[0], 1, 3)\n",
    "del train_loader, val_loader, test_loader, train_image_batch, val_image_batch, test_image_batch, train_noise_image_batch, val_noise_image_batch, test_noise_image_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiICDe1EvQi9"
   },
   "source": [
    "# Define our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uko92WJe-vcy"
   },
   "outputs": [],
   "source": [
    "class MyNeuralNetRegressor(nn.Module):\n",
    "  # We create a neural network with 1 hidden layers with 1000 neurons, \n",
    "  # and an output layer with 784 neurons (to match the dimension of our images!)\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetRegressor, self).__init__()\n",
    "        # These are the layers. fc1 is the first feed-forward/fully connedted layer etc\n",
    "        # Here we are just defining layers but the way that they will be put together is determined in the forward function.\n",
    "        self.fc1 = nn.Linear(28*28, 1000) # input dimension is 784 (28*28 is the number of pixels) and the layer has 1000 neurons\n",
    "        self.fc2 = nn.Linear(1000, 28*28)  # input dimension is 1000 (from previous layer) and the layer has 784 neurons/\n",
    "                                         # We use 784 neurons so the output dimension matches the input dimension.\n",
    "\n",
    "    # This defines the forward pass of the network; the input is passed through each layer, and the nonlinearity is applied.\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)            # We 'flatten' the image to be of dimensions (#images, 784) instead of (#images, 1, 28, 28)\n",
    "        x = torch.relu(self.fc1(x))      # Pass the input through linear layer and apply ReLU to outputs.\n",
    "        x = torch.relu(self.fc2(x))      # This is the output layer (again, linear + ReLU)\n",
    "        x = x.view(x.shape[0],1,28,28)   # 'unflatten' output data from dimensions (#images, 784) to (#images, 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNlruqePwNMN"
   },
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPPyK-qEvZdo"
   },
   "outputs": [],
   "source": [
    "# We can use the free GPUs offered by Colab\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') #GPU if available (cuda represents GPU)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "  \n",
    "# Verifying which device is being used GPU vs. CPU \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOEIYVgcEqnZ"
   },
   "outputs": [],
   "source": [
    "# A placeholder function.\n",
    "# Implement the function when work on the Task1 Q1.\n",
    "def plot_eval_results(train_report, test_loss):\n",
    "  print('!to be implemented')\n",
    "\n",
    "# Net is the NN model, e.g. MyNeuralNetRegressor.\n",
    "# Criterion is the loss function we use.\n",
    "# Epochs is the number of times that model will be trained with the whole dataset, \n",
    "# it controls the outter loop of the training process.\n",
    "# Learning rate is the step size of optimization algorithm (stochastic gradient descent e.g.). \n",
    "# We may fail to learn if learning rate is too big or too small. \n",
    "# You may try play with different rate values, e.g. 1, 0.1, 0.001, 0.0001. \n",
    "def main(config, net, criterion, drop_probability=0.7, epochs=10, batch_size=64, split_ratio=0.5, learning_rate=0.05):\n",
    "  # Here the task is to \"denoise\" images. In particular,\n",
    "  # the input of the regression algorithm is a noisy \n",
    "  # picture of a digit, and the regressor should reconstruct\n",
    "  # the original image of the digit. To do the training,\n",
    "  # we create a data set where the inputs are noisy \n",
    "  # digits and the outputs are the original images.\n",
    "  # The learning method is then tested on new unseen\n",
    "  # noisy digits. This is done using the test set.\n",
    "\n",
    "  #create a fresh model\n",
    "  NeuralNet = net()\n",
    "\n",
    "  # Define optimizer for learning weights(parameters). \n",
    "  # Adam and SGD are two commonly used optimizers.\n",
    "  # We use Adam here but you can update it to SGD.\n",
    "  optimizer = optim.Adam(NeuralNet.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_loader, val_loader, test_loader = load_data(path=config['data_path'],\n",
    "                drop_probability=drop_probability, split_ratio=split_ratio, batch_size=batch_size)\n",
    "  print(f'Successfully loaded data')\n",
    "\n",
    "  # Train the network. \n",
    "  # Return the network with updated weights(parameters). \n",
    "  # train_report contains information for analyzing train/validation loss. \n",
    "  NeuralNet, train_report = train(train_loader, val_loader, NeuralNet, epochs, criterion, optimizer, config['model_path'])\n",
    "  print('Training finished')\n",
    "\n",
    "  # Load the best model.\n",
    "  NeuralNet.load_state_dict(torch.load(config['model_path']))\n",
    "\n",
    "  # Validate with test dataset and return test loss.\n",
    "  test_loss = test(test_loader, NeuralNet, criterion)\n",
    "\n",
    "  print(f'Test loss is {test_loss} for drop_probability={drop_probability}, epochs={epochs}, batch_size={batch_size}, split_ratio={split_ratio}, learning_rate={learning_rate}')\n",
    "\n",
    "  # Visualize the train/validation/test loss.\n",
    "  plot_eval_results(train_report, test_loss)\n",
    "\n",
    "  return NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRRX_XQBvqXA"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asHFZ9DRyPdO"
   },
   "outputs": [],
   "source": [
    "# This is our training function.\n",
    "# train_loader contains training image pairs.\n",
    "# val_loader stores validation image pairs.\n",
    "def train(train_loader, val_loader, model, epochs, loss_function, optimizer, model_path, print_loss=True):\n",
    "  best_loss = None\n",
    "  report = []\n",
    "\n",
    "  # Loop over the entire dataset #epochs times.\n",
    "  for epoch in range(epochs):  \n",
    "    print(f\"epoch {epoch}\")\n",
    "\n",
    "    # Set model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Average loss between a pair of denoised image and original image. \n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    report_single={\n",
    "        train_loss: None,\n",
    "        val_loss: None\n",
    "    }\n",
    "\n",
    "    for train_image_batch, train_noise_image_batch in iter(train_loader):\n",
    "      # Move model and data to GPU.\n",
    "      model = model.to(device)\n",
    "      train_noise_image_batch = train_noise_image_batch[0].to(device)\n",
    "      train_image_batch = train_image_batch[0].to(device)\n",
    "\n",
    "      # Below, we update weights(parameters).\n",
    "      # We pass the inputs through the model, compute the loss, and backpropogate the error.\n",
    "        \n",
    "      # Pytorch optimizer accumulates gradient values.\n",
    "      # Zero out the gradient before backpropogate.\n",
    "      optimizer.zero_grad()\n",
    "             \n",
    "      # Forward pass. Compute output.\n",
    "      denoised_images = model(train_noise_image_batch)\n",
    "      loss = loss_function(denoised_images, train_image_batch)\n",
    "\n",
    "      # Backward pass. Compute gradients.\n",
    "      loss.backward()\n",
    "      \n",
    "      # Optimize.\n",
    "      optimizer.step()\n",
    "\n",
    "      # Add the average loss of the batch to the total loss.\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    # Calculate the average training loss for the whole training dataset.\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    report_single['train_loss'] = train_loss\n",
    "\n",
    "    if print_loss == True:\n",
    "      print(f\"train_loss={train_loss}\")\n",
    "\n",
    "    # Validate model with validation set.\n",
    "    # Disable gradiant since we are not updating weights during validation.\n",
    "    with torch.no_grad():\n",
    "      for val_image_batch, val_noise_image_batch in iter(val_loader):\n",
    "        val_image_batch = val_image_batch[0].to(device)\n",
    "        val_noise_image_batch = val_noise_image_batch[0].to(device)\n",
    "\n",
    "        denoised_images = model(val_noise_image_batch)\n",
    "\n",
    "        loss = loss_function(denoised_images, val_image_batch)\n",
    "          \n",
    "        val_loss += loss.item()\n",
    "\n",
    "      # Calculate the average loss for the whole validation dataset.\n",
    "      val_loss = val_loss/len(val_loader)\n",
    "      report_single['val_loss'] = val_loss\n",
    "\n",
    "      if print_loss == True:\n",
    "        print(f\"val_loss={val_loss}\") \n",
    "\n",
    "      # If validation loss is better than the current best loss,\n",
    "      # save it as the best loss and save the model as the best model.\n",
    "      if best_loss is None or (val_loss < best_loss):\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    report.append(report_single)\n",
    "\n",
    "  # Return the model and report. \n",
    "  # The model is the last updated model but may not be the best model saved.\n",
    "  return model, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbiffchUI7AC"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuiavZYTI_xz"
   },
   "outputs": [],
   "source": [
    "# Run test against test dataset.\n",
    "def test(dataloader, model, loss_function):\n",
    "  # This is our test function. We pass all the testing data\n",
    "  # through the Network and compute the loss.\n",
    "  test_loss = 0.0\n",
    "  model.to(device)\n",
    "\n",
    "  # Set model to eval mode.\n",
    "  model.eval()\n",
    "\n",
    "  for image_batch, noise_image_batch in iter(dataloader):\n",
    "    # Move data to GPU\n",
    "    image_batch = image_batch[0].to(device)\n",
    "    noise_image_batch = noise_image_batch[0].to(device)\n",
    "\n",
    "    denoised_images = model(noise_image_batch)\n",
    "    loss = loss_function(denoised_images, image_batch)\n",
    "    test_loss += loss.item()\n",
    "\n",
    "  test_loss = test_loss/len(dataloader)\n",
    " \n",
    "  return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMfFDuU788t3"
   },
   "source": [
    "# Run our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3czw4o2W_Gr"
   },
   "outputs": [],
   "source": [
    "# Set up config variables\n",
    "config = {\n",
    "    'data_path': './data', # directory of dataset,\n",
    "    'model_path': './checkpoints/best_model_1.pt' # directory saving the best model\n",
    "}\n",
    "# create folder checkpoints to save model\n",
    "!mkdir -p checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx7FaGMj9A_5"
   },
   "outputs": [],
   "source": [
    "# Set up objective function (we use L1 loss or absolute loss function function)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Train and test model\n",
    "model = main(config, net=MyNeuralNetRegressor, criterion=criterion, epochs=50, \n",
    "             learning_rate=0.05, drop_probability=0.7, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTzW28aJwERP"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epDSDmMFworx"
   },
   "source": [
    "**Q1 (25 points)**. In this question we want to draw a graph that shows the training loss and the validation loss throughout the traning. Finish the implementation of functin plot_eval_results based on the following requirements:\n",
    "\n",
    "1.   You can use plot function from matplotlib library.\n",
    "2.   X-axis is the epochs number\n",
    "3.   Y-axis corresponds loss.\n",
    "4.   Draw two graphs for train loss and validation loss on the same plot.\n",
    "5.   Draw a horizontal line for test loss (it is basically the test loss at the epoch with the best validation loss)\n",
    "\n",
    "After the implementation, run `main(config, criterion=nn.L1Loss(), net=MyNeuralNetRegressor, epochs=50, drop_probability=0.7, learning_rate=0.05, batch_size=64)`.\n",
    "\n",
    "Include the graph in your report. Justify the trend of the graphs.\n",
    "\n",
    "An example graph is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KuqE3zAtxEm"
   },
   "source": [
    "![report.jpg](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW9vj78jsm2X"
   },
   "outputs": [],
   "source": [
    "def plot_eval_results(train_report, test_loss):\n",
    "  # Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAd-thi-soGV"
   },
   "outputs": [],
   "source": [
    "# Run your model training. \n",
    "criterion = nn.L1Loss()\n",
    "model = main(config, criterion=criterion, net=MyNeuralNetRegressor, epochs=50, drop_probability=0.7, learning_rate=0.05, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sS1HcMSpO-g"
   },
   "source": [
    "**Q2 (10 points)**. How many parameters does MyNeuralNetRegressor have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0eXhbNClWX5"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjg7lD2XoMkl"
   },
   "source": [
    "Convolutional Neural Network(CNN) have been quite successful in the field of image processing. Let us implement a basic CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilqfh4r4p0xG"
   },
   "source": [
    "We can replace fully connected layers in MyNeuralNetRegressor with convolutional layers.\n",
    "A convolutional layer can be implemented in pytorch like\n",
    "\n",
    "```\n",
    "nn.Conv2d(in_channels=, out_channels=, kernel_size=, padding=, stride=)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKgDJRMWqYZl"
   },
   "source": [
    "**Q1 (25 points)**. Finish implementing MyCNN based on the description. Train the network with the same configuration, i.e., \n",
    "\n",
    "`main(config, MyCNN, criterion=nn.L1Loss(), drop_probability=0.7, learning_rate=0.05, epochs=50, batch_size=64)`\n",
    "\n",
    "Draw the loss graph for both train, validation, and test (like the previous part) for the implemented CNN. Include the graph in your report and compare the results with the previous part (and justify them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-ZdaiANgf2R"
   },
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "  # We create a 2 layers CNN. \n",
    "  # First conv is configured with filters(kernels) size 3, padding 1, and stride 1.\n",
    "  # The first conv's inputs are images in dimension 28*28*1 (W*H*1). \n",
    "  # Its output is in dimension 28*28*10 (W1*H1*num_feature_maps), where\n",
    "      # W1=(W-Filter+2*Padding)/Stride+1 => 28=(28-3+2)/1+1.\n",
    "      # H1=(H-Filter+2*Padding)/Stride+1 => 28=(28-3+2)/1+1.\n",
    "  # Apply ReLU to the first conv output.\n",
    "  # Pass the result as input to the second conv layer.\n",
    "  # The second conv is configured with padding 1 and stride 1. \n",
    "    # You will need to figure out the filter size for the second conv.\n",
    "  # Apply ReLU to the second conv output.\n",
    "  # The final outputs are denoised images (dimension 28*28*1).\n",
    "      \n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "  \n",
    "        self.num_input_channels = 1 # number of input image channel, \n",
    "                                # 1 for grayscale images, 3 for color images\n",
    "        self.num_feature_maps = 10 # number of feature maps\n",
    "        self.num_output_channels = 1 # number of output image channel\n",
    "            \n",
    "        self.kernel_size = 3\n",
    "        self.padding = 1\n",
    "        self.stride = 1\n",
    "\n",
    "        # Add your conv layers below\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7BHvcicgsdQ"
   },
   "outputs": [],
   "source": [
    "# Run your model training.\n",
    "model = main(config, MyCNN, criterion=nn.L1Loss(), drop_probability=0.7, learning_rate=0.05, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvVWYRre0OPV"
   },
   "source": [
    "**Q2 (20 points)**. How many parameters does MyCNN have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-b2IQup1Yf1"
   },
   "source": [
    "**Q3 (10 points)**. Denoise 10 images using MyNeuralNetworkRegressor and MyCNN separately. Display the denoised images and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpXz0FYFuRIV"
   },
   "source": [
    "# Task 3\n",
    "**Q1 (30 points)**. In this part the goal is to improve the performance of MyCNN. We still want to use the same drop_probability, same training/validation size, and the same loss function. However, you can change the architecture and/or parameters of the network.\n",
    "\n",
    "```\n",
    "main(config, MySuperNN, drop_probability=0.7, criterion=, learning_rate=, epochs=, batch_size=)\n",
    "```\n",
    "\n",
    "Include the changes that you made in your report. Explain the intuition behind your modifications.\n",
    "\n",
    "Also, draw the graphs for train/test/validation similar to Task 1. How much improvement did you get in your test loss compared to MyCNN?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iiICDe1EvQi9",
    "lNlruqePwNMN",
    "KRRX_XQBvqXA",
    "YbiffchUI7AC"
   ],
   "name": "Assignment_4_2021_WIN_4MLS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
